"""llama_run.ipynb
Automatically generated by Colaboratory.
"""

# SOURCE: https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/blob/main/Run%20Llama2%20Google%20Colab/Llama_2_updated.ipynb
# Set up required packages
!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose
!pip install huggingface_hub
!pip install llama-cpp-python==0.1.78
!pip install numpy==1.23.4

# Download and load the model
model_name_or_path = "TheBloke/Llama-2-7B-chat-GGML"
model_basename = "llama-2-7b-chat.ggmlv3.q5_1.bin"

# Import require libraries
from huggingface_hub import hf_hub_download
from llama_cpp import Llama
model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)

# Load the model
lcpp_llm = None
lcpp_llm = Llama(
    model_path=model_path,
    n_threads=2,
    n_batch=1024, 
    n_gpu_layers=32
    )
    
# Variables setup
input_file = 'promapen_llama.csv'
output_file = 'results.txt'
language = 'en' # cz en
responses = []
# Run the model and the data and predict labels
import pandas as pd
labels = []
df=pd.read_csv(input_file, sep='\t', encoding='utf-8')
data =df[['name1', 'name2']].values

if language=='cz':
        prompt = f"Jsou tyto dva produkty stejné? Napiš 1 pokud ano a 0 pokud ne. \n PRVNI PRODUKT: {data[i][0]} \n DRUHY PRODUKT: {data[i][1]}"
    else:
        prompt = f"Are these two products the same? Print 1 if yes or 0 if no. \n FIRST PRODUCT: {data[i][0]} \n SECOND PRODUCT: {data[i][1]}"

prompt_template=f'''SYSTEM: Answer honestly.
USER: {prompt}
ASSISTANT:
'''

response=lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,
              repeat_penalty=1.2, top_k=150,
              echo=True)
print(response["choices"][0]["text"])
responses.append(response["choices"][0]["text"])

# Store results
with open(output_file, 'w', encoding='utf-8') as f:
    for i in responses:
        f.write(i)